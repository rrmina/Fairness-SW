{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 0\n",
    "# Load word2vec data from a file handle\n",
    "def load_word2vec_format(f, max_num_words=None):\n",
    "    \"\"\"Loads word2vec data from a file handle.\n",
    "\n",
    "    Similar to gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "    but takes a file handle as input rather than a filename. This lets us use\n",
    "    GFile. Also only accepts binary files.\n",
    "\n",
    "    Args:\n",
    "        f: file handle\n",
    "        max_num_words: number of words to load. If None, load all.\n",
    "\n",
    "    Returns:\n",
    "        Word2vec data as keyedvectors.EuclideanKeyedVectors.\n",
    "    \"\"\"\n",
    "    # Print Vocab Size and Vector Size\n",
    "    header = f.readline()\n",
    "    vocab_size, vector_size = (int(x) for x in header.rstrip().split())  # throws for invalid file format\n",
    "    print(\"vocab_size = {}, vector_size =  {}\".format(vocab_size, vector_size))\n",
    "\n",
    "    # Instantiate gensim model\n",
    "    result = gensim.models.keyedvectors.EuclideanKeyedVectors()\n",
    "    result.vector_size = vector_size\n",
    "    result.syn0 = np.zeros( (vocab_size, vector_size), dtype=np.float32)\n",
    "     \n",
    "    num_words = 0\n",
    "\n",
    "    # Function: Add word\n",
    "    def add_word(word, weights):\n",
    "        word_id = len(result.vocab)\n",
    "        if word in result.vocab:\n",
    "            print(\"duplicate word '{}', ignoring all but first\".format(word))\n",
    "            return\n",
    "        result.vocab[word] = gensim.models.keyedvectors.Vocab(index=word_id, count=vocab_size - word_id)\n",
    "        result.syn0[word_id] = weights\n",
    "        result.index2word.append(word)\n",
    "\n",
    "    # Print how many words are being loaded\n",
    "    if max_num_words and max_num_words < vocab_size:\n",
    "        num_embeddings = max_num_words\n",
    "    else:\n",
    "        num_embeddings = vocab_size\n",
    "    print(\"Loading {} embeddings\".format(num_embeddings))\n",
    "    \n",
    "    binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "    for _ in range(vocab_size):\n",
    "        \n",
    "        if (DEBUG):\n",
    "            if (num_words % 200000 == 0):\n",
    "                print(num_words)\n",
    "\n",
    "        # mixed text and binary: read text first, then binary\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == b' ':\n",
    "                break\n",
    "            if ch == b'':\n",
    "                raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "            if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "                word.append(ch)\n",
    "\n",
    "        word = gensim.utils.to_unicode(b''.join(word), encoding='utf-8', errors='strict')\n",
    "        weights = np.frombuffer(f.read(binary_len), dtype=np.float32)\n",
    "        add_word(word, weights)\n",
    "        num_words = num_words + 1\n",
    "        if max_num_words and num_words == max_num_words:\n",
    "            break\n",
    "\n",
    "    if result.syn0.shape[0] != len(result.vocab):\n",
    "        print(\"duplicate words detected, shrinking matrix size from {} to {}\".format(result.syn0.shape[0], len(result.vocab)))\n",
    "    result.syn0 = np.ascontiguousarray(result.syn0[:len(result.vocab)])\n",
    "    assert (len(result.vocab), vector_size) == result.syn0.shape\n",
    "\n",
    "    print(\"loaded {} matrix\".format(result.syn0.shape))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the candidate words for A - B + C\n",
    "def print_knn(client, v, k):\n",
    "    print(\"{} closest neighbors to C - A + B\".format(k))\n",
    "    for neighbor, score in client.similar_by_vector(v.flatten().astype(float), topn=k):\n",
    "        print(\"{} : score={}\".format(neighbor, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analogies from a file\n",
    "def load_analogies(filename):\n",
    "    \"\"\"\n",
    "    Returns a list containing the analogies\n",
    "    \"\"\"\n",
    "    analogies = []\n",
    "    with open(filename, 'r') as fast_file:\n",
    "        for line in fast_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip comment lines (i.e. lines the start with ':')\n",
    "            if (line[0] == ':'):\n",
    "                continue\n",
    "            \n",
    "            # Ignore lines without exactly 4 words\n",
    "            words = line.split()\n",
    "            if (len(words) != 4):\n",
    "                print(\"Invalid line -> {}\".format(line))\n",
    "                continue\n",
    "\n",
    "            analogies.append(words)\n",
    "\n",
    "    print(\"Loaded {} analogies\".format(len(analogies)))\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 3000000, vector_size =  300\n",
      "Loading 3000000 embeddings\n",
      "loaded (3000000, 300) matrix\n"
     ]
    }
   ],
   "source": [
    "# Load word embeddings\n",
    "WORD2VEC_FILE = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "MAX_NUM_WORDS = None\n",
    "with gzip.GzipFile(fileobj = open(WORD2VEC_FILE, 'rb')) as f:\n",
    "    client = load_word2vec_format(f, MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest neighbors to C - A + B\n",
      "programmer : score=0.8918285965919495\n",
      "programmers : score=0.5779235363006592\n",
      "programer : score=0.5624995827674866\n",
      "Programmer : score=0.5415105819702148\n",
      "sysadmin : score=0.5366033911705017\n",
      "Jon_Shiring : score=0.5260592699050903\n",
      "coder : score=0.5256212949752808\n",
      "modder : score=0.4957827031612396\n",
      "animator : score=0.4935148358345032\n",
      "engineer : score=0.4899101257324219\n"
     ]
    }
   ],
   "source": [
    "# Word analogy example: A : B :: C : D\n",
    "#  ->  D = C - A + B\n",
    "NUM_ANALOGIES = 10\n",
    "\n",
    "A = \"woman\"\n",
    "B = \"man\"\n",
    "C = \"programmer\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest neighbors to C - A + B\n",
      "programmer : score=0.885962188243866\n",
      "programmers : score=0.6040860414505005\n",
      "computer_programmer : score=0.5623368620872498\n",
      "coder : score=0.5616979598999023\n",
      "Programmer : score=0.5576066374778748\n",
      "programer : score=0.5161396861076355\n",
      "graphic_designer : score=0.5139066576957703\n",
      "coders : score=0.4876540005207062\n",
      "designer : score=0.4822674095630646\n",
      "librarian : score=0.4649229645729065\n"
     ]
    }
   ],
   "source": [
    "# Word analogy example: A : B :: C : D\n",
    "#  ->  D = C - A + B\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"programmer\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest neighbors to C - A + B\n",
      "doctor : score=0.883492112159729\n",
      "gynecologist : score=0.7276507019996643\n",
      "nurse : score=0.6698512434959412\n",
      "physician : score=0.6674121022224426\n",
      "doctors : score=0.6649492979049683\n",
      "pediatrician : score=0.6398377418518066\n",
      "nurse_practitioner : score=0.6237459778785706\n",
      "obstetrician : score=0.6188926696777344\n",
      "midwife : score=0.6041982769966125\n",
      "dentist : score=0.5999662280082703\n"
     ]
    }
   ],
   "source": [
    "# Word analogy example: A : B :: C : D\n",
    "#  ->  D = C - A + B\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"doctor\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest neighbors to C - A + B\n",
      "doctor : score=0.8413019180297852\n",
      "physician : score=0.6823903918266296\n",
      "doctors : score=0.6239282488822937\n",
      "surgeon : score=0.5908077955245972\n",
      "dentist : score=0.570309042930603\n",
      "cardiologist : score=0.5666105151176453\n",
      "neurologist : score=0.5558009743690491\n",
      "neurosurgeon : score=0.5432174801826477\n",
      "internist : score=0.5405333042144775\n",
      "urologist : score=0.5398820042610168\n"
     ]
    }
   ],
   "source": [
    "# Word analogy example: A : B :: C : D\n",
    "#  ->  D = C - A + B\n",
    "A = \"woman\"\n",
    "B = \"man\"\n",
    "C = \"doctor\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest neighbors to C - A + B\n",
      "programmer : score=0.885962188243866\n",
      "programmers : score=0.6040860414505005\n",
      "computer_programmer : score=0.5623368620872498\n",
      "coder : score=0.5616979598999023\n",
      "Programmer : score=0.5576066374778748\n",
      "programer : score=0.5161396861076355\n",
      "graphic_designer : score=0.5139066576957703\n",
      "coders : score=0.4876540005207062\n",
      "designer : score=0.4822674095630646\n",
      "librarian : score=0.4649229645729065\n"
     ]
    }
   ],
   "source": [
    "# Word analogy example: A : B :: C : D\n",
    "#  ->  D = C - A + B\n",
    "A = \"man\"\n",
    "B = \"programmer\"\n",
    "C = \"woman\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19544 analogies\n"
     ]
    }
   ],
   "source": [
    "# Load analogies\n",
    "ANALOGIES_FILE = \"questions-words.txt\"\n",
    "analogies = load_analogies(ANALOGIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athens is to Greece as Baghdad is to Iraq\n",
      "Athens is to Greece as Bangkok is to Thailand\n",
      "Athens is to Greece as Beijing is to China\n",
      "Athens is to Greece as Berlin is to Germany\n",
      "Athens is to Greece as Bern is to Switzerland\n"
     ]
    }
   ],
   "source": [
    "# Print some analogies\n",
    "for x in analogies[:5]:\n",
    "    print(\"{} is to {} as {} is to {}\".format(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the input vector normalized\n",
    "def normalize_vector(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "# Load and return normalized vectors\n",
    "def load_embeddings(client, analogies):\n",
    "\n",
    "    # Get and count the unique words in analogies\n",
    "    # i.e. update = Union operation\n",
    "    words_unfiltered = set()\n",
    "    for i in range(len(analogies)):\n",
    "        words_unfiltered.update(analogies[i])\n",
    "\n",
    "    print(\"Found {} unique words\".format(len(words_unfiltered)))\n",
    "\n",
    "    # Normalize word vectors\n",
    "    words = []\n",
    "    vect = []\n",
    "    index_map = {}\n",
    "    for word in words_unfiltered:\n",
    "        try:\n",
    "            vect.append(normalize_vector(client.word_vec(word)))\n",
    "            index_map[word] = len(words)\n",
    "            words.append(word)\n",
    "        except KeyError:\n",
    "            print(\"Word not found: {}\".format(word))\n",
    "\n",
    "    print(\"Number of words not filtered out: {}/{}\".format(len(words), len(words_unfiltered)))\n",
    "\n",
    "    return np.array(vect), index_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 905 unique words\n",
      "Number of words not filtered out: 905/905\n"
     ]
    }
   ],
   "source": [
    "embeddings, index_map, word_list = load_embeddings(client, analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gender_direction(embed,\n",
    "                          indices):\n",
    "  \"\"\"Finds and returns a 'gender direction'.\"\"\"\n",
    "  pairs = [\n",
    "      (\"woman\", \"man\"),\n",
    "      (\"her\", \"his\"),\n",
    "      (\"she\", \"he\"),\n",
    "      (\"aunt\", \"uncle\"),\n",
    "      (\"niece\", \"nephew\"),\n",
    "      (\"daughters\", \"sons\"),\n",
    "      (\"mother\", \"father\"),\n",
    "      (\"daughter\", \"son\"),\n",
    "      (\"granddaughter\", \"grandson\"),\n",
    "      (\"girl\", \"boy\"),\n",
    "      (\"stepdaughter\", \"stepson\"),\n",
    "      (\"mom\", \"dad\"),\n",
    "  ]\n",
    "  m = []\n",
    "  for wf, wm in pairs:\n",
    "    m.append(embed[indices[wf]] - embed[indices[wm]])\n",
    "  m = np.array(m)\n",
    "\n",
    "  # the next three lines are just a PCA.\n",
    "  m = np.cov(np.array(m).T)\n",
    "  evals, evecs = np.linalg.eig(m)\n",
    "  return normalize_vector(np.real(evecs[:, np.argmax(evals)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "gender_direction = find_gender_direction(embeddings, index_map)\n",
    "#print(\"gender direction: %s\" % str(gender_direction.flatten()))\n",
    "print(gender_direction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.30643988058688776\n"
     ]
    }
   ],
   "source": [
    "WORD = \"master\"\n",
    "\n",
    "word_vec = client.word_vec(WORD)\n",
    "print(word_vec.dot(gender_direction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  gender_score\n",
      "864           his     -0.660903\n",
      "792            he     -0.584860\n",
      "442  unimpressive     -0.544877\n",
      "684       Anaheim     -0.503144\n",
      "629         Libya     -0.486664\n",
      "428       playing     -0.472259\n",
      "670          play     -0.459254\n",
      "450      sharpest     -0.455905\n",
      "734       Detroit     -0.454509\n",
      "576        calmly     -0.448576\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "words = set()\n",
    "for a in analogies:\n",
    "  words.update(a)\n",
    "\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"gender_score\"] = df[\"word\"].map(\n",
    "    lambda w: client.word_vec(w).dot(gender_direction))\n",
    "df.sort_values(by=\"gender_score\", inplace=True)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  gender_score\n",
      "201        queen      0.682354\n",
      "162          her      0.683995\n",
      "417      stepson      0.686814\n",
      "409      sisters      0.699692\n",
      "680          she      0.706431\n",
      "350     princess      0.719736\n",
      "76           mom      0.732802\n",
      "344        women      0.758530\n",
      "702  policewoman      0.816518\n",
      "489      husband      0.950914\n"
     ]
    }
   ],
   "source": [
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  gender_score\n",
      "489      husband      0.950914\n",
      "702  policewoman      0.816518\n",
      "344        women      0.758530\n",
      "76           mom      0.732802\n",
      "350     princess      0.719736\n",
      "680          she      0.706431\n",
      "409      sisters      0.699692\n",
      "417      stepson      0.686814\n",
      "162          her      0.683995\n",
      "201        queen      0.682354\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=\"gender_score\", inplace=True, ascending=False)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       " ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       " ['Athens', 'Greece', 'Beijing', 'China'],\n",
       " ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       " ['Athens', 'Greece', 'Bern', 'Switzerland']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        bias = self.fc1(x)\n",
    "        out = identity - bias\n",
    "        return out\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class AdversarialModel(nn.Module):\n",
    "    def __init__(self, embedding_size, gender_direction, v, y, z, word2vec, device):\n",
    "        super(AdversarialModel, self).__init__()\n",
    "\n",
    "        # Global Settings and Hyperparameters\n",
    "        self.num_epochs = 10000\n",
    "        self.device = device\n",
    "        self.adversary_lr = 0.01\n",
    "        self.predictor_lr = 0.01\n",
    "\n",
    "        # Dataset\n",
    "        self.v = v.to(device)\n",
    "        self.y = v.to(device)\n",
    "        self.z = z.to(device)\n",
    "\n",
    "        # Variables\n",
    "        self.gender_direction = gender_direction.to(device)\n",
    "\n",
    "        # Layers\n",
    "        self.predictor = Predictor(embedding_size).to(device)\n",
    "        self.adversary = Adversary(embedding_size).to(device)\n",
    "\n",
    "        # Loss Criterion\n",
    "        self.adversary_criterion = nn.MSELoss().to(device)\n",
    "        self.predictor_criterion = nn.MSELoss().to(device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.adversary_optim = optim.Adam(self.adversary.parameters(), lr=1e-1)\n",
    "        self.predictor_optim = optim.Adam(self.predictor.parameters(), lr=1.5e-5)\n",
    "\n",
    "        # WORD2VEC Model\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def adversary_forward(self, x):\n",
    "        x = self.predictor(x) # Predict the 4th word\n",
    "        x = self.adversary(x) # Predict the gender direction based on the 4th word\n",
    "        return x\n",
    "\n",
    "    def predictor_forward(self, x):\n",
    "        x = self.predictor(x)\n",
    "        return x\n",
    "\n",
    "    def adversary_loss(self, predicted_gender, gender_direction):\n",
    "        return self.adversary_criterion(predicted_gender, gender_direction)\n",
    "\n",
    "    def predictor_loss(self, predicted_d, d):\n",
    "        return self.predictor_criterion(predicted_d, d)\n",
    "\n",
    "    def train(self):\n",
    "        # Always turn on train mode\n",
    "        self.predictor.train()\n",
    "        self.adversary.train()\n",
    "\n",
    "        # Prepare Dataset\n",
    "        v = self.v\n",
    "        y = self.y\n",
    "        z = self.z.float()\n",
    "\n",
    "        for i in range(self.num_epochs):\n",
    "            # Adversary Training\n",
    "            self.adversary_optim.zero_grad()        # Zero-out gradients\n",
    "            z_hat = self.adversary_forward(v)       # Adversary Forward Pass\n",
    "            La = self.adversary_loss(z_hat, z)      # Adversary Loss\n",
    "            La.backward()                           # Backprop\n",
    "            self.adversary_optim.step()             # Gradient Descent\n",
    "\n",
    "            # Predictor Training\n",
    "            self.predictor_optim.zero_grad()        # Zero-out gradients\n",
    "            y_hat = self.predictor_forward(v)       # Predictor Forward Pass\n",
    "            Lp = self.predictor_loss(y_hat, y)      # Predictor Loss\n",
    "            \n",
    "            z_hat_p = self.adversary_forward(y_hat) # Predictor -> Adversary Forward Pass\n",
    "            Lp_a = self.adversary_loss(z_hat_p, z)  # Adversary Loss\n",
    "            Lp_total = 300 * Lp - Lp_a                     # Total Predictor Loss\n",
    "            Lp_total.backward()                     # Backprop\n",
    "            self.predictor_optim.step()             # Gradient Descent\n",
    "\n",
    "            # Print Losses\n",
    "            if ((i == 0) or ((i+1) % 50 == 0)):\n",
    "                print(\"La: {} - Lp_p: {} - Lp_a: {} - Lp_total: {}\".format(La.item(), Lp.item() * 300, Lp_a.item(), Lp_total.item()))\n",
    "    \n",
    "    def analogy(self, A, B, C):\n",
    "        # Load the embeddings of each word\n",
    "        A_vec = self.word2vec.word_vec(A)\n",
    "        B_vec = self.word2vec.word_vec(B)\n",
    "        C_vec = self.word2vec.word_vec(C)\n",
    "        \n",
    "        v = C_vec - A_vec + B_vec\n",
    "        v = torch.from_numpy(v).to(self.device)\n",
    "\n",
    "        self.predictor.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_d = self.predictor(v)\n",
    "\n",
    "        pred_d = pred_d.cpu().numpy()\n",
    "\n",
    "        def print_knn(embeddings, v, k):\n",
    "            print(\"{} closest neighbors to C - A + B\".format(k))\n",
    "            for neighbor, score in embeddings.similar_by_vector(v.flatten().astype(float), topn=k):\n",
    "                print(\"{} : score={}\".format(neighbor, score))\n",
    "\n",
    "        print_knn(self.word2vec, pred_d, k=10)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word analogy -> A : B :: C : D\n",
    "# Input to the Adversarial Training is predicted D = C - A + B\n",
    "# Label is D\n",
    "def process_sample(analogy, embed, index_map, gender_direction):\n",
    "    A, B, C, D = analogy\n",
    "    A = embed[index_map[A]]\n",
    "    B = embed[index_map[B]]\n",
    "    C = embed[index_map[C]]\n",
    "    D = embed[index_map[D]]\n",
    "    gender_proj = np.dot(D, gender_direction)\n",
    "    return  C-A+B, D, gender_proj\n",
    "\n",
    "# Add analogy entries\n",
    "v_train = []\n",
    "y = []\n",
    "z = []\n",
    "gender_direction_normed = normalize_vector(gender_direction)\n",
    "for analogy in analogies:\n",
    "    v_sample, y_sample, z_sample = process_sample(analogy, embeddings, index_map, gender_direction_normed)\n",
    "    v_train.append(v_sample)\n",
    "    y.append(y_sample)\n",
    "    z.append(z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "v_train = torch.from_numpy(np.array(v_train))\n",
    "y = torch.from_numpy(np.array(y))\n",
    "z = torch.from_numpy(np.array(z)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19544, 300])\n",
      "torch.Size([19544, 300])\n",
      "torch.Size([19544, 1])\n"
     ]
    }
   ],
   "source": [
    "print(v_train.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Adversarial Model\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_size = client.vector_size\n",
    "gender_torch = torch.from_numpy(gender_direction)\n",
    "model = AdversarialModel(embedding_size, gender_torch, v_train, y, z, client, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La: 0.006924546789377928 - Lp_p: 0.8360947715118527 - Lp_a: 1.493820071220398 - Lp_total: -0.6577252745628357\n",
      "La: 0.0036067701876163483 - Lp_p: 0.8076941594481468 - Lp_a: 0.0027467445470392704 - Lp_total: 0.8049473762512207\n",
      "La: 0.0013885240769013762 - Lp_p: 0.7583159487694502 - Lp_a: 0.002327305730432272 - Lp_total: 0.755988597869873\n",
      "La: 0.0013095267349854112 - Lp_p: 0.7097869645804167 - Lp_a: 0.002500282134860754 - Lp_total: 0.7072866559028625\n",
      "La: 0.0012843497097492218 - Lp_p: 0.6644613342359662 - Lp_a: 0.0027361956890672445 - Lp_total: 0.6617251038551331\n",
      "La: 0.001273082452826202 - Lp_p: 0.6229062331840396 - Lp_a: 0.0029474797192960978 - Lp_total: 0.6199586987495422\n",
      "La: 0.0012677587801590562 - Lp_p: 0.585155573207885 - Lp_a: 0.0031176512129604816 - Lp_total: 0.5820378661155701\n",
      "La: 0.0012651649303734303 - Lp_p: 0.5510137067176402 - Lp_a: 0.0032546138390898705 - Lp_total: 0.5477591156959534\n",
      "La: 0.0012638868065550923 - Lp_p: 0.5201873718760908 - Lp_a: 0.0033698228653520346 - Lp_total: 0.516817569732666\n",
      "La: 0.001263267477042973 - Lp_p: 0.4923517000861466 - Lp_a: 0.0034734634682536125 - Lp_total: 0.4888782501220703\n",
      "La: 0.0012629835400730371 - Lp_p: 0.46718308003619313 - Lp_a: 0.003573077730834484 - Lp_total: 0.4636099934577942\n",
      "La: 0.0012628685217350721 - Lp_p: 0.4443758516572416 - Lp_a: 0.003674008185043931 - Lp_total: 0.44070184230804443\n",
      "La: 0.0012628341792151332 - Lp_p: 0.4236504784785211 - Lp_a: 0.0037802178412675858 - Lp_total: 0.4198702573776245\n",
      "La: 0.0012628332478925586 - Lp_p: 0.4047559225000441 - Lp_a: 0.003894926980137825 - Lp_total: 0.400860995054245\n",
      "La: 0.0012628408148884773 - Lp_p: 0.3874703077599406 - Lp_a: 0.0040210094302892685 - Lp_total: 0.3834492862224579\n",
      "La: 0.0012628447730094194 - Lp_p: 0.37159931380301714 - Lp_a: 0.0041612302884459496 - Lp_total: 0.36743807792663574\n",
      "La: 0.0012628402328118682 - Lp_p: 0.35697470884770155 - Lp_a: 0.0043183877132833 - Lp_total: 0.3526563346385956\n",
      "La: 0.001262826262973249 - Lp_p: 0.3434513113461435 - Lp_a: 0.004495407920330763 - Lp_total: 0.33895590901374817\n",
      "La: 0.001262804726138711 - Lp_p: 0.3309052437543869 - Lp_a: 0.004695412237197161 - Lp_total: 0.3262098431587219\n",
      "La: 0.0012627774849534035 - Lp_p: 0.31923099886626005 - Lp_a: 0.0049217464402318 - Lp_total: 0.3143092691898346\n",
      "La: 0.0012627473333850503 - Lp_p: 0.3083392046391964 - Lp_a: 0.005177982151508331 - Lp_total: 0.30316123366355896\n",
      "La: 0.0012627163669094443 - Lp_p: 0.298154738266021 - Lp_a: 0.0054678781889379025 - Lp_total: 0.2926868498325348\n",
      "La: 0.0012626865645870566 - Lp_p: 0.2886140369810164 - Lp_a: 0.005795286502689123 - Lp_total: 0.2828187346458435\n",
      "La: 0.0012626590905711055 - Lp_p: 0.27966368361376226 - Lp_a: 0.006163996644318104 - Lp_total: 0.2734996974468231\n",
      "La: 0.0012626351090148091 - Lp_p: 0.2712581539526582 - Lp_a: 0.006577493157237768 - Lp_total: 0.2646806538105011\n",
      "La: 0.001262615085579455 - Lp_p: 0.26335803559049964 - Lp_a: 0.0070386347360908985 - Lp_total: 0.25631940364837646\n",
      "La: 0.0012625992530956864 - Lp_p: 0.25592847377993166 - Lp_a: 0.007549242116510868 - Lp_total: 0.24837924540042877\n",
      "La: 0.0012625872623175383 - Lp_p: 0.24893697118386626 - Lp_a: 0.008109597489237785 - Lp_total: 0.24082736670970917\n",
      "La: 0.0012625791132450104 - Lp_p: 0.2423520607408136 - Lp_a: 0.008717941120266914 - Lp_total: 0.2336341142654419\n",
      "La: 0.00126257399097085 - Lp_p: 0.2361415943596512 - Lp_a: 0.009369973093271255 - Lp_total: 0.22677162289619446\n",
      "La: 0.0012625715462490916 - Lp_p: 0.2302719047293067 - Lp_a: 0.010058518499135971 - Lp_total: 0.220213383436203\n",
      "La: 0.0012625707313418388 - Lp_p: 0.22470701951533556 - Lp_a: 0.010773420333862305 - Lp_total: 0.21393360197544098\n",
      "La: 0.0012625707313418388 - Lp_p: 0.21940864389762282 - Lp_a: 0.011501781642436981 - Lp_total: 0.20790687203407288\n",
      "La: 0.0012625710805878043 - Lp_p: 0.2143370860721916 - Lp_a: 0.012228538282215595 - Lp_total: 0.20210854709148407\n",
      "La: 0.0012625710805878043 - Lp_p: 0.2094521652907133 - Lp_a: 0.012937433086335659 - Lp_total: 0.19651474058628082\n",
      "La: 0.0012625708477571607 - Lp_p: 0.2047152549494058 - Lp_a: 0.013612107373774052 - Lp_total: 0.19110314548015594\n",
      "La: 0.0012625698000192642 - Lp_p: 0.2000908018089831 - Lp_a: 0.014237301424145699 - Lp_total: 0.1858534961938858\n",
      "La: 0.001262567937374115 - Lp_p: 0.19554775790311396 - Lp_a: 0.01479983702301979 - Lp_total: 0.18074792623519897\n",
      "La: 0.001262565841898322 - Lp_p: 0.19106075051240623 - Lp_a: 0.01528935506939888 - Lp_total: 0.17577140033245087\n",
      "La: 0.001262563280761242 - Lp_p: 0.18661044887267053 - Lp_a: 0.01569865643978119 - Lp_total: 0.1709117889404297\n",
      "La: 0.0012625607196241617 - Lp_p: 0.18218342447653413 - Lp_a: 0.016023408621549606 - Lp_total: 0.16616001725196838\n",
      "La: 0.0015672596637159586 - Lp_p: 0.17777052707970142 - Lp_a: 0.01269620843231678 - Lp_total: 0.16507431864738464\n",
      "La: 0.001264982856810093 - Lp_p: 0.17338385805487633 - Lp_a: 0.01663331501185894 - Lp_total: 0.15675054490566254\n",
      "La: 0.0012625721283257008 - Lp_p: 0.1690125383902341 - Lp_a: 0.016541915014386177 - Lp_total: 0.15247061848640442\n",
      "La: 0.0012625525705516338 - Lp_p: 0.1646488846745342 - Lp_a: 0.01652178354561329 - Lp_total: 0.14812710881233215\n",
      "La: 0.00126255105715245 - Lp_p: 0.1603011740371585 - Lp_a: 0.01645413041114807 - Lp_total: 0.1438470482826233\n",
      "La: 0.0015856563113629818 - Lp_p: 0.15596429002471268 - Lp_a: 0.021640775725245476 - Lp_total: 0.13432350754737854\n",
      "La: 0.0012670074356719851 - Lp_p: 0.15169940306805074 - Lp_a: 0.015819352120161057 - Lp_total: 0.13588005304336548\n",
      "La: 0.0012625698000192642 - Lp_p: 0.14747506356798112 - Lp_a: 0.015903515741229057 - Lp_total: 0.13157154619693756\n",
      "La: 0.001262547681108117 - Lp_p: 0.1432849676348269 - Lp_a: 0.015659887343645096 - Lp_total: 0.12762507796287537\n",
      "La: 0.001262547099031508 - Lp_p: 0.13914309383835644 - Lp_a: 0.015351788140833378 - Lp_total: 0.12379130721092224\n",
      "La: 0.0012626469833776355 - Lp_p: 0.13503548980224878 - Lp_a: 0.012308194302022457 - Lp_total: 0.12272728979587555\n",
      "La: 0.0012632205616682768 - Lp_p: 0.13103755773045123 - Lp_a: 0.014988642185926437 - Lp_total: 0.1160489171743393\n",
      "La: 0.0012625480303540826 - Lp_p: 0.12712797615677118 - Lp_a: 0.014318922534584999 - Lp_total: 0.11280905455350876\n",
      "La: 0.0012625461677089334 - Lp_p: 0.12327801086939871 - Lp_a: 0.013930930756032467 - Lp_total: 0.10934708267450333\n",
      "La: 0.001262545702047646 - Lp_p: 0.11950233019888401 - Lp_a: 0.013544568791985512 - Lp_total: 0.1059577614068985\n",
      "La: 0.0033339676447212696 - Lp_p: 0.11577394616324455 - Lp_a: 0.008384634740650654 - Lp_total: 0.10738930851221085\n",
      "La: 0.0012692136224359274 - Lp_p: 0.11215443955734372 - Lp_a: 0.012839569710195065 - Lp_total: 0.0993148684501648\n",
      "La: 0.0012625742238014936 - Lp_p: 0.10868289973586798 - Lp_a: 0.012344182468950748 - Lp_total: 0.09633871912956238\n",
      "La: 0.0012625453528016806 - Lp_p: 0.1052809733664617 - Lp_a: 0.012004262767732143 - Lp_total: 0.09327670931816101\n",
      "La: 0.0012625453528016806 - Lp_p: 0.10196338116656989 - Lp_a: 0.011637384071946144 - Lp_total: 0.09032599627971649\n",
      "La: 0.001265666913241148 - Lp_p: 0.09873619128484279 - Lp_a: 0.011525647714734077 - Lp_total: 0.08721054345369339\n",
      "La: 0.0012735486961901188 - Lp_p: 0.09547676600050181 - Lp_a: 0.010473561473190784 - Lp_total: 0.08500320464372635\n",
      "La: 0.0012627519899979234 - Lp_p: 0.09248701971955597 - Lp_a: 0.010634572245180607 - Lp_total: 0.08185245096683502\n",
      "La: 0.001262546400539577 - Lp_p: 0.08957107784226537 - Lp_a: 0.010259432718157768 - Lp_total: 0.07931164652109146\n",
      "La: 0.001262545003555715 - Lp_p: 0.08673369884490967 - Lp_a: 0.009959271177649498 - Lp_total: 0.07677442580461502\n",
      "La: 0.001262545003555715 - Lp_p: 0.08398263598792255 - Lp_a: 0.009676407091319561 - Lp_total: 0.07430623471736908\n",
      "La: 0.0022375790867954493 - Lp_p: 0.08131933864206076 - Lp_a: 0.007966636680066586 - Lp_total: 0.07335270196199417\n",
      "La: 0.001328930607996881 - Lp_p: 0.07856363954488188 - Lp_a: 0.009630340151488781 - Lp_total: 0.06893330067396164\n",
      "La: 0.0012629111297428608 - Lp_p: 0.07612429617438465 - Lp_a: 0.008867337368428707 - Lp_total: 0.06725695729255676\n",
      "La: 0.0012625467497855425 - Lp_p: 0.07374973793048412 - Lp_a: 0.00862065888941288 - Lp_total: 0.06512907892465591\n",
      "La: 0.001262545119971037 - Lp_p: 0.07144715928006917 - Lp_a: 0.008426358923316002 - Lp_total: 0.06302079558372498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La: 0.001262545003555715 - Lp_p: 0.06922089960426092 - Lp_a: 0.008241433650255203 - Lp_total: 0.06097946688532829\n",
      "La: 0.001262545119971037 - Lp_p: 0.06707058346364647 - Lp_a: 0.008069878444075584 - Lp_total: 0.05900070071220398\n",
      "La: 0.0017082135891541839 - Lp_p: 0.0648549321340397 - Lp_a: 0.007538123521953821 - Lp_total: 0.05731680989265442\n",
      "La: 0.0012764051789417863 - Lp_p: 0.06280556990532205 - Lp_a: 0.007547618355602026 - Lp_total: 0.05525795370340347\n",
      "La: 0.0012625723611563444 - Lp_p: 0.060914154164493084 - Lp_a: 0.007522429805248976 - Lp_total: 0.053391724824905396\n",
      "La: 0.001262545003555715 - Lp_p: 0.05908080056542531 - Lp_a: 0.007416368927806616 - Lp_total: 0.051664434373378754\n",
      "La: 0.001262545003555715 - Lp_p: 0.05730918928747997 - Lp_a: 0.007318942341953516 - Lp_total: 0.04999024420976639\n",
      "La: 0.001262545003555715 - Lp_p: 0.05559974815696478 - Lp_a: 0.007230608724057674 - Lp_total: 0.048369139432907104\n",
      "La: 0.001262545003555715 - Lp_p: 0.053951206791680306 - Lp_a: 0.007151687517762184 - Lp_total: 0.046799518167972565\n",
      "La: 0.0019769032951444387 - Lp_p: 0.05236043070908636 - Lp_a: 0.0083855539560318 - Lp_total: 0.043974876403808594\n",
      "La: 0.0012971797259524465 - Lp_p: 0.05070607439847663 - Lp_a: 0.006883253343403339 - Lp_total: 0.043822821229696274\n",
      "La: 0.0012630561832338572 - Lp_p: 0.04923620290355757 - Lp_a: 0.006879452615976334 - Lp_total: 0.04235675185918808\n",
      "La: 0.0012625475646927953 - Lp_p: 0.04784868797287345 - Lp_a: 0.006844418589025736 - Lp_total: 0.04100426658987999\n",
      "La: 0.001262545003555715 - Lp_p: 0.046509796811733395 - Lp_a: 0.006823529954999685 - Lp_total: 0.03968627005815506\n",
      "La: 0.001262545119971037 - Lp_p: 0.0452188920462504 - Lp_a: 0.0068075270392000675 - Lp_total: 0.03841136395931244\n",
      "La: 0.001262545003555715 - Lp_p: 0.04397481243358925 - Lp_a: 0.006797206588089466 - Lp_total: 0.03717760741710663\n",
      "La: 0.001262545003555715 - Lp_p: 0.0427762177423574 - Lp_a: 0.0067924573086202145 - Lp_total: 0.03598376363515854\n",
      "La: 0.001262545119971037 - Lp_p: 0.04162174591328949 - Lp_a: 0.00679254624992609 - Lp_total: 0.03482919931411743\n",
      "La: 0.0022207952570170164 - Lp_p: 0.04065196408191696 - Lp_a: 0.007198741193860769 - Lp_total: 0.03345322608947754\n",
      "La: 0.001271914690732956 - Lp_p: 0.03950030368287116 - Lp_a: 0.00688008451834321 - Lp_total: 0.03262021765112877\n",
      "La: 0.0012626060051843524 - Lp_p: 0.03848824562737718 - Lp_a: 0.006856093183159828 - Lp_total: 0.03163215517997742\n",
      "La: 0.0012625452363863587 - Lp_p: 0.037522825004998595 - Lp_a: 0.006877053529024124 - Lp_total: 0.03064577281475067\n",
      "La: 0.001262545119971037 - Lp_p: 0.03659278081613593 - Lp_a: 0.006909117102622986 - Lp_total: 0.029683664441108704\n",
      "La: 0.001262545119971037 - Lp_p: 0.03569650652934797 - Lp_a: 0.006943375803530216 - Lp_total: 0.0287531316280365\n",
      "La: 0.001262545119971037 - Lp_p: 0.03483285181573592 - Lp_a: 0.006979989353567362 - Lp_total: 0.02785286121070385\n",
      "La: 0.001262545003555715 - Lp_p: 0.03400077548576519 - Lp_a: 0.007018887437880039 - Lp_total: 0.026981890201568604\n",
      "La: 0.001262545003555715 - Lp_p: 0.03319931493024342 - Lp_a: 0.007059600669890642 - Lp_total: 0.02613971382379532\n",
      "La: 0.005866531748324633 - Lp_p: 0.032684445613995194 - Lp_a: 0.00690943468362093 - Lp_total: 0.025775011628866196\n",
      "La: 0.0012639759806916118 - Lp_p: 0.03200001738150604 - Lp_a: 0.007497656159102917 - Lp_total: 0.024502359330654144\n",
      "La: 0.0012626596726477146 - Lp_p: 0.03127521194983274 - Lp_a: 0.007346013095229864 - Lp_total: 0.02392919920384884\n",
      "La: 0.001262545003555715 - Lp_p: 0.03059325899812393 - Lp_a: 0.007416627369821072 - Lp_total: 0.023176632821559906\n",
      "La: 0.001262545003555715 - Lp_p: 0.029937056388007477 - Lp_a: 0.007459668442606926 - Lp_total: 0.022477388381958008\n",
      "La: 0.001262545119971037 - Lp_p: 0.029304798954399303 - Lp_a: 0.0075056543573737144 - Lp_total: 0.02179914340376854\n",
      "La: 0.001262545119971037 - Lp_p: 0.02869569871108979 - Lp_a: 0.007552519906312227 - Lp_total: 0.021143179386854172\n",
      "La: 0.001262545003555715 - Lp_p: 0.028109144477639347 - Lp_a: 0.007600333075970411 - Lp_total: 0.020508810877799988\n",
      "La: 0.001262545003555715 - Lp_p: 0.027544588374439627 - Lp_a: 0.007649242412298918 - Lp_total: 0.019895344972610474\n",
      "La: 0.0012863214360550046 - Lp_p: 0.02736800306593068 - Lp_a: 0.014924325980246067 - Lp_total: 0.01244367752224207\n",
      "La: 0.0012948737712576985 - Lp_p: 0.02707442472456023 - Lp_a: 0.008440839126706123 - Lp_total: 0.018633585423231125\n",
      "La: 0.0012625462841242552 - Lp_p: 0.02652478142408654 - Lp_a: 0.008108556270599365 - Lp_total: 0.018416225910186768\n",
      "La: 0.0012625455856323242 - Lp_p: 0.026022155361715704 - Lp_a: 0.008163891732692719 - Lp_total: 0.017858263105154037\n",
      "La: 0.001262545003555715 - Lp_p: 0.02554261445766315 - Lp_a: 0.008194442838430405 - Lp_total: 0.017348172143101692\n",
      "La: 0.001262545003555715 - Lp_p: 0.02508375109755434 - Lp_a: 0.00822834949940443 - Lp_total: 0.016855400055646896\n",
      "La: 0.001262545003555715 - Lp_p: 0.02464474891894497 - Lp_a: 0.008264775387942791 - Lp_total: 0.01637997478246689\n",
      "La: 0.001262545119971037 - Lp_p: 0.024224988010246307 - Lp_a: 0.008303661830723286 - Lp_total: 0.015921324491500854\n",
      "La: 0.001262545003555715 - Lp_p: 0.02382384627708234 - Lp_a: 0.008344831876456738 - Lp_total: 0.015479014255106449\n",
      "La: 0.001262545003555715 - Lp_p: 0.023440701625077054 - Lp_a: 0.008388074114918709 - Lp_total: 0.01505262777209282\n",
      "La: 0.0025884946808218956 - Lp_p: 0.02307893300894648 - Lp_a: 0.007417665328830481 - Lp_total: 0.015661267563700676\n",
      "La: 0.0012810563202947378 - Lp_p: 0.023353959841188043 - Lp_a: 0.009414849802851677 - Lp_total: 0.013939110562205315\n",
      "La: 0.0012628805125132203 - Lp_p: 0.02290589254698716 - Lp_a: 0.008926361799240112 - Lp_total: 0.013979529961943626\n",
      "La: 0.0012625473318621516 - Lp_p: 0.02254932333016768 - Lp_a: 0.008868396282196045 - Lp_total: 0.013680927455425262\n",
      "La: 0.001262545119971037 - Lp_p: 0.022217768855625764 - Lp_a: 0.00888248160481453 - Lp_total: 0.01333528757095337\n",
      "La: 0.001262545003555715 - Lp_p: 0.02190660816268064 - Lp_a: 0.008909686468541622 - Lp_total: 0.012996922247111797\n",
      "La: 0.001262545119971037 - Lp_p: 0.021614042634610087 - Lp_a: 0.008941316045820713 - Lp_total: 0.012672726996243\n",
      "La: 0.001262545003555715 - Lp_p: 0.021338638180168346 - Lp_a: 0.008976816199719906 - Lp_total: 0.012361821718513966\n",
      "La: 0.001262545003555715 - Lp_p: 0.021079133148305118 - Lp_a: 0.009015503339469433 - Lp_total: 0.012063628993928432\n",
      "La: 0.001262545003555715 - Lp_p: 0.02083440776914358 - Lp_a: 0.009056739509105682 - Lp_total: 0.011777669191360474\n",
      "La: 0.00128756754565984 - Lp_p: 0.020603695884346962 - Lp_a: 0.008784448727965355 - Lp_total: 0.011819247156381607\n",
      "La: 0.0013138180365785956 - Lp_p: 0.02072400675388053 - Lp_a: 0.008319206535816193 - Lp_total: 0.012404799461364746\n",
      "La: 0.001263657584786415 - Lp_p: 0.02043746571871452 - Lp_a: 0.00928546767681837 - Lp_total: 0.011151998303830624\n",
      "La: 0.0012625523377209902 - Lp_p: 0.020217925339238718 - Lp_a: 0.009369690902531147 - Lp_total: 0.01084823440760374\n",
      "La: 0.001262545119971037 - Lp_p: 0.020017739734612405 - Lp_a: 0.009407678619027138 - Lp_total: 0.010610060766339302\n",
      "La: 0.001262545003555715 - Lp_p: 0.019832770340144634 - Lp_a: 0.0094420425593853 - Lp_total: 0.01039072871208191\n",
      "La: 0.001262545003555715 - Lp_p: 0.019661067926790565 - Lp_a: 0.009479778818786144 - Lp_total: 0.010181288234889507\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy with Word2vec (reduced bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.analogy(\"man\", \"woman\", \"friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy with Word2vec (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"friend\"\n",
    "A_vec = client.word_vec(A)\n",
    "B_vec = client.word_vec(B)\n",
    "C_vec = client.word_vec(C)\n",
    "D_vec = C_vec - A_vec + B_vec\n",
    "\n",
    "print_knn(client, D_vec, NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
